import requests
from bs4 import BeautifulSoup
import time

def get_defaced_domains_from_page(page_number):
    """
    TrÃ­ch xuáº¥t cÃ¡c domain bá»‹ deface tá»« má»™t trang cá»¥ thá»ƒ cá»§a Zone-H.
    """
    # URL cá»§a trang lÆ°u trá»¯, page=X Ä‘á»ƒ chuyá»ƒn trang
    url = f"https://zone-h.org/archive/page={page_number}"
    domains = []
    
    try:
        # Gá»­i yÃªu cáº§u vá»›i User-Agent Ä‘á»ƒ giáº£ láº­p trÃ¬nh duyá»‡t
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status() # BÃ¡o lá»—i náº¿u request khÃ´ng thÃ nh cÃ´ng

        soup = BeautifulSoup(response.text, 'html.parser')
        
        # TÃ¬m táº¥t cáº£ cÃ¡c hÃ ng <tr> trong báº£ng, bá» qua hÃ ng tiÃªu Ä‘á» Ä‘áº§u tiÃªn
        # Dá»±a trÃªn cáº¥u trÃºc HTML cá»§a Zone-H, cÃ¡c hÃ ng dá»¯ liá»‡u khÃ´ng cÃ³ thuá»™c tÃ­nh 'class'
        table_rows = soup.find_all('tr', onmouseover=True)
        
        for row in table_rows:
            # CÃ¡c Ã´ dá»¯ liá»‡u trong hÃ ng lÃ  <td>
            cells = row.find_all('td')
            if len(cells) > 3:
                # Domain náº±m á»Ÿ Ã´ thá»© 4 (index 3) vÃ  chá»©a má»™t tháº» <a>
                domain_cell = cells[3]
                domain_text = domain_cell.get_text(strip=True)
                
                # LÃ m sáº¡ch domain, loáº¡i bá» pháº§n "..." vÃ  cÃ¡c kÃ½ tá»± khÃ´ng mong muá»‘n
                if '...' in domain_text:
                    full_domain_link = domain_cell.find('a')
                    if full_domain_link and 'title' in full_domain_link.attrs:
                        domain_text = full_domain_link['title']
                
                # ThÃªm http:// Ä‘á»ƒ táº¡o thÃ nh URL hoÃ n chá»‰nh
                full_url = f"http://{domain_text}"
                domains.append(full_url)

    except requests.exceptions.RequestException as e:
        print(f"Lá»—i khi truy cáº­p trang {page_number}: {e}")
        
    return domains

# --- CHáº Y CÃ”NG Cá»¤ ---
if __name__ == "__main__":
    all_defaced_urls = []
    # VÃ­ dá»¥: láº¥y dá»¯ liá»‡u tá»« 5 trang Ä‘áº§u tiÃªn
    for page in range(1, 6): 
        print(f"Äang quÃ©t trang {page}...")
        urls_on_page = get_defaced_domains_from_page(page)
        if not urls_on_page:
            print(f"KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u hoáº·c Ä‘Ã£ háº¿t trang.")
            break
        
        all_defaced_urls.extend(urls_on_page)
        
        # ThÃªm Ä‘á»™ trá»… 1-2 giÃ¢y giá»¯a cÃ¡c láº§n request Ä‘á»ƒ trÃ¡nh bá»‹ cháº·n
        time.sleep(1.5)

    # LÆ°u káº¿t quáº£ vÃ o file
    with open('defaced_urls.txt', 'w') as f:
        for url in all_defaced_urls:
            f.write(f"{url}\n")
            
    print(f"\nğŸ‰ HoÃ n táº¥t! ÄÃ£ trÃ­ch xuáº¥t vÃ  lÆ°u {len(all_defaced_urls)} URL vÃ o file defaced_urls.txt")