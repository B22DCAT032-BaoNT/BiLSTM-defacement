import requests
from bs4 import BeautifulSoup
import os
import re

def clean_filename(url):
    """Táº¡o tÃªn file an toÃ n tá»« URL."""
    # Loáº¡i bá» scheme (http, https)
    if url.startswith(('http://', 'https://')):
        url = url[url.find('//')+2:]
    
    # Thay tháº¿ cÃ¡c kÃ½ tá»± khÃ´ng há»£p lá»‡ báº±ng dáº¥u gáº¡ch dÆ°á»›i
    return re.sub(r'[\\/*?:"<>|]', "_", url)

def extract_text_from_url(url, session):
    """
    Táº£i HTML tá»« má»™t URL, trÃ­ch xuáº¥t vÃ  tráº£ vá» ná»™i dung vÄƒn báº£n thuáº§n.
    """
    try:
        # Sá»­ dá»¥ng headers Ä‘á»ƒ giáº£ láº­p má»™t trÃ¬nh duyá»‡t thÃ´ng thÆ°á»ng
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        # Táº£i trang web vá»›i timeout lÃ  15 giÃ¢y
        response = session.get(url, timeout=15, headers=headers)
        # BÃ¡o lá»—i náº¿u yÃªu cáº§u khÃ´ng thÃ nh cÃ´ng
        response.raise_for_status()

        # Sá»­ dá»¥ng BeautifulSoup Ä‘á»ƒ phÃ¢n tÃ­ch HTML
        soup = BeautifulSoup(response.content, 'html.parser')

        # Loáº¡i bá» cÃ¡c tháº» <script> vÃ  <style> vÃ¬ chÃºng chá»©a mÃ£, khÃ´ng pháº£i ná»™i dung 
        for script_or_style in soup(['script', 'style']):
            script_or_style.decompose()

        # Láº¥y vÄƒn báº£n tá»« ná»™i dung cÃ²n láº¡i
        text = soup.get_text()

        # Dá»n dáº¹p vÄƒn báº£n: loáº¡i bá» dÃ²ng trá»‘ng vÃ  khoáº£ng tráº¯ng thá»«a
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        cleaned_text = '\n'.join(chunk for chunk in chunks if chunk)

        return cleaned_text

    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ Lá»—i khi xá»­ lÃ½ URL {url}: {e}")
        return None

def process_url_list(input_file, output_dir):
    """
    Äá»c danh sÃ¡ch URL tá»« file, trÃ­ch xuáº¥t vÄƒn báº£n vÃ  lÆ°u vÃ o thÆ° má»¥c Ä‘áº§u ra.
    """
    print(f"\nğŸš€ Báº¯t Ä‘áº§u xá»­ lÃ½ file: {input_file}")
    
    # Táº¡o thÆ° má»¥c Ä‘áº§u ra náº¿u chÆ°a tá»“n táº¡i
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ÄÃ£ táº¡o thÆ° má»¥c: {output_dir}")

    # Äá»c danh sÃ¡ch URLs tá»« file
    try:
        with open(input_file, 'r') as f:
            urls = [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file '{input_file}'. Vui lÃ²ng kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n.")
        return

    if not urls:
        print("File URL trá»‘ng, khÃ´ng cÃ³ gÃ¬ Ä‘á»ƒ xá»­ lÃ½.")
        return

    # Sá»­ dá»¥ng session Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng káº¿t ná»‘i, tÄƒng hiá»‡u suáº¥t
    with requests.Session() as session:
        for i, url in enumerate(urls):
            print(f"   ({i+1}/{len(urls)}) Äang láº¥y dá»¯ liá»‡u tá»«: {url}")
            
            # TrÃ­ch xuáº¥t vÄƒn báº£n
            plain_text = extract_text_from_url(url, session)

            if plain_text:
                # Táº¡o tÃªn file tá»« URL vÃ  lÆ°u káº¿t quáº£
                file_name = f"{clean_filename(url)}.txt"
                output_path = os.path.join(output_dir, file_name)
                
                try:
                    with open(output_path, 'w', encoding='utf-8') as out_file:
                        out_file.write(plain_text)
                    # print(f"      âœ… ÄÃ£ lÆ°u thÃ nh cÃ´ng vÃ o: {output_path}")
                except Exception as e:
                    print(f"      âŒ Lá»—i khi lÆ°u file {output_path}: {e}")

    print(f"ğŸ‰ HoÃ n táº¥t xá»­ lÃ½ {len(urls)} URLs tá»« {input_file}.")

# --- Cáº¤U HÃŒNH VÃ€ CHáº Y ---
if __name__ == "__main__":
    # 1. Dá»¯ liá»‡u cho cÃ¡c trang web bÃ¬nh thÆ°á»ng
    NORMAL_URLS_FILE = 'normal_urls.txt'
    NORMAL_OUTPUT_DIR = 'data/normal_text'
    process_url_list(NORMAL_URLS_FILE, NORMAL_OUTPUT_DIR)

    # 2. Dá»¯ liá»‡u cho cÃ¡c trang web bá»‹ thay Ä‘á»•i giao diá»‡n
    DEFACED_URLS_FILE = 'defaced_urls.txt'
    DEFACED_OUTPUT_DIR = 'data/defaced_text'
    process_url_list(DEFACED_URLS_FILE, DEFACED_OUTPUT_DIR)